---
layout: post
title: Spectral Clustering
---

In this blog post, we will be studying spectral clustering. Clustering is a great way to do exploratory analysis. The goal of clustering is to divide the data points into several group so that points in one group are more similar to each other than points in another group.

One reason why spectral clustering is so useful is because it treats the data clustering as a graph partitioning problem and does not make any assumptions on the form of the data clusters like other clustering algorithms, such as K-means. Here's an example of what we mean by that: 

<br> 
Below, is a randomly generated data set tweaked so that we can see 2 distinct "blobs":
```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![output_2_1.png](/images/output_2_1.png)

In this data set, K-means clustering works well because the two groups are circular. 
```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![output_4_1.png](/images/output_4_1.png)

## Why do we need spectral clustering?
However, when the data points do not group nicely together in a circular form, K-means fails to group the data points in the way that we would expect it to. We can see that clearly happening here with this new data:
```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
![output_4_1.png](/images/output_6_1.png)

Given this data, this is what the K-means clustering technique will output:
```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![output_4_1.png](/images/output_8_1.png)

We see above that K-means doesn't properly separate the data points, which seem to be separated in a crescent shape. This is where spectral clustering comes in! <br>

## Defining spectral clustering
According to [Wikipedia's definition](https://en.wikipedia.org/wiki/Spectral_clustering), "spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions." Let's break down this definition.


## Similarity Matrix 
Given an enumerated set of data points, the similarity matrix $$A$$ is symmetric, where $$A_{i,j}$$  represents a measure of the similarity between data points with indices $$i$$ and $$j$$. One way to measure the similarity between any two data points is by computing the pairwise distance between them. Once we calculate the pairwise distance between all data points, we can use a parameter called `epsilon` to determine if these two data points are actually similar. Given `epsilon` we can construct the similarity matrix such that for every entry $$A_{i,j}$$ in the similarity matrix $$A$$, $$A_{i,j}$$ is equal to 1 if $$X_i$$ is within the distance, `epsilon` of $$X_j$$. Otherwise, $$A_{i,j}$$ is equal to 0. Additionally, all diagonal entries, $$A_{i,i}$$ is equal to 0 since the pairwise distance between the point and itself is 0. <br>


Let's calculate the similarity matrix for the data points in this plot using `epsilon = 0.4`:
![output_4_1.png](/images/output_6_1.png)

```python
from sklearn.metrics import pairwise_distances
# initialize epsilon 
epsilon = 0.4
# calculate the distance between each datapoint in X
A = pairwise_distances(X, metric="euclidean")
# determine if each entry in A is less than epsilon^2
A = (A < epsilon ** 2).astype(int)
# fill the diagonal entries of A with 0 
np.fill_diagonal(A, 0)
# display our similarity matrix 
A
```
    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           ...,
           [0, 0, 0, ..., 0, 0, 1],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 1, 0, 0]])



## Binary Norm Cut Objective
The similarity matrix $$A$$ contains information on which points are near each other, but we still don't know which points belong to what group. Thus, out next task is to cluster the data points in `X` by partitioning the rows and columns of $$A$$. We can do this by finding the binary norm cut objects of $$A$$. In math terms, <br>
<p align="center">
$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$
</p>

### Cut Term
In the binary norm cut object, the cut term is referred to as $$\mathbf{cut}(C_0, C_1)$$ and is the *cut* of the clusters $$C_0$$ and $$C_1$$. Mathematically, 
<p align="center">
$$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$
</p>
What this expression means is that the cut is the sum of all weights between data points that connect one data point in cluster $$C_0$$ to cluster $$C_1$$. 

```python
def cut(A, y):
    """
    Calculates the cut of two clusters given a similarity matrix,
    A, and cluster membership, y.
    """
    # array that holds the indices of y 
    point_index = np.array(range(len(y)))
    # get the index of each data point according to its cluster 
    C1_index = point_index[y==1]
    C0_index = point_index[y==0]
    
    # add the entries from the similarity matrix for each pair of 
    # points (i,j) in different clusters
    cut_sum = 0
    for i in C1_index:
        for j in C0_index:
            cut_sum += A[i, j]
           
    return cut_sum
```
When we say that the cut term is small, it means that points in $$C_0$$ are not close to points in $$C_1$$. We can see this is the case when we compute the cut objective for the true clusters `y`. 
```python
cut(A, y)
``` 
   0
   
However, when we generate a random vector with the clustering labels, the cut is no longer small. 
```python
# generate random cluster labels
random_y = np.random.choice([0, 1], size=200)
# calculate the cut 
cut(A, random_y)
```
   387
<br> 
### The Volume Term
The second factor in the norm cut objective is the *volume term*. 
$$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). 

What the expression above means is that the *volume* of cluster $$C_0$$ is a measure of the size of the cluster. Using this definition of *volume*, we can easily define a function to calculate its value.
```python
def vols(A, y):
    """
    Calculates the volume of clusters given a similarity matrix, 
    A, and clustering labels, y. 
    """
    # calculate the degree of row i 
    d = np.sum(A, axis=1)
    # volume of C0 and C1 cluster
    vol_C0 = np.sum((y == 0) * d)
    vol_C1 = np.sum((y == 1) * d)
    return vol_C0, vol_C1
```

## Calculating the binary norm cut objective
Now that we have our cut term and volume term. We can finally calculate the binary norm cut objective. 

```python
def normcut(A, y):
    # cut term
    cut_val = cut(A,y)
    # volumes of each cluster
    v0, v1 = vols(A, y)
    return cut_val * (1/v0 + 1/v1)
```
Norm cut objective on true labels: 
```python
normcut(A, y)
```




    0.0



Norm cut objective on random labels:
```python
normcut(A, random_y)
```




    0.9941491209783893


We see that the normcut for the true labels is smaller than the normcut for the fake labels. While this is great,  finding a cluster vector `y` such that `normcut(A, y)` is small  is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We can use the following math trick such that: 
<p align = "center">
$$\mathbf{z} \in \mathbb{R}^n$$
$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$
</p>

Now, we can define the normal cut objective to be: 
<p align = "center">
$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$
</p>

1. Write a function called `transform(A,y) t`o compute the appropriate $$z$$ vector given `A` and `y`, using the formula above.
```python
def transform(A, y):
    z = np.zeros(len(y))
    v0, v1 = vols(A, y)
    z[y==0] = 1 / v0
    z[y==1] = -1/ v1
    return z
```


2. Then, check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal.
```python
d_ = np.sum(A, axis=1)
z = transform(A, y)
D = (np.eye(len(d_))) * d_
assert np.isclose(normcut(A,y), (z.T@(D - A)@z) / (z.T@D@z))
```
3. While you're here, also check the identity  ùê≥ùëáùêÉùüô=0 , where  ùüô  is the vector of n ones (i.e. np.ones(n)). This identity effectively says that  ùê≥  should contain roughly as many positive as negative entries.
```python
assert np.isclose(z@D@np.ones(200), 0)
```

**side note to peers: wasn't feeling well right around here so just inserted code for the different sections of the assignment, but will work on explanatory text and commenting code more for parts D onwards**
## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. In the code below, I define an `orth_obj` function which handles this for you. 

Use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name `z_`. 

```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```
```python
from scipy.optimize import minimize, LinearConstraint

z_ = minimize(orth_obj,x0=z, constraints=(LinearConstraint(D@np.ones(len(D)), 0, 0, keep_feasible=False)))
```
## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

Does it look like we came close to correctly clustering the data? 
```python
plt.scatter(X[:,0], X[:,1], c = z_.x < 0)
```
![output_31_1.png](/images/output_31_1.png)

## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color. How did we do? 


```python
L = np.linalg.inv(D)@(D-A)
eig_values, eig_vectors = np.linalg.eig(L)
second_smallest_idx = np.argsort(eig_values, axis=-1)[1]
z_eig = eig_vectors[:, second_smallest_idx] 
```


```python
plt.scatter(X[:,0], X[:,1], c = z_eig < 0 )
```
![png](/images/output_34_1.png)
    


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

## Part G

Synthesize your results from the previous parts. In particular, write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. Demonstrate your function using the supplied data from the beginning of the problem. 

#### Notes

Despite the fact that this has been a long journey, the final function should be quite short. You should definitely aim to keep your solution under 10, very compact lines. 

**In this part only, please supply an informative docstring!** 

#### Outline

Given data, you need to: 

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def get_similarity_matrix(X, epsilon):
    A = pairwise_distances(X, metric="euclidean")
    A = (A < epsilon ** 2).astype(int)
    np.fill_diagonal(A, 0)
    return A

def get_laplacian(A):
    d_ = np.sum(A, axis=1)
    D = np.eye(len(d_)) * d_
    laplacian = np.linalg.pinv(D)@(D-A)
    return laplacian

def get_eigen_vector(laplacian):
    _eig_values, _eig_vectors = np.linalg.eig(laplacian)
    idx = np.argsort(_eig_values, axis=-1)[1]
    _z_eig = _eig_vectors[:, idx] 
    return _z_eig

def spectral_clustering(X, epsilon):
    A = get_similarity_matrix(X, epsilon)
    L = get_laplacian(A)
    _z_eig = get_eigen_vector(L)
    return _z_eig
```

## Part H

Run a few experiments using your function, by generating different data sets using `make_moons`. What happens when you increase the `noise`? Does spectral clustering still find the two half-moon clusters? For these experiments, you may find it useful to increase `n` to `1000` or so -- we can do this now, because of our fast algorithm! 


```python
np.random.seed(1)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.075, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    
![png](/images/output_39_1.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon=0.48))
```


    
![png](/images/output_40_1.png)
    


## Part I

Now try your spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

    
![png](/images/output_42_1.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![png](/images/output_44_1.png)
    


Can your function successfully separate the two circles? Some experimentation here with the value of `epsilon` is likely to be required. Try values of `epsilon` between `0` and `1.0` and describe your findings. For roughly what values of `epsilon` are you able to correctly separate the two rings? 


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon=0.4))
```


    
![png](/images/output_46_1.png)
    

