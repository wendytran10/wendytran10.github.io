---
layout: post
title: Spectral Clustering
---

In this blog post, we will be studying spectral clustering. Clustering is a great way to do exploratory analysis. The goal of clustering is to divide the data points into several group so that points in one group are more similar to each other than points in another group.

One reason why spectral clustering is so useful is because it treats the data clustering as a graph partitioning problem and does not make any assumptions on the form of the data clusters like other clustering algorithms, such as K-means. Here's an example of what we mean by that: 

<br> 
Below, is a randomly generated data set tweaked so that we can see 2 distinct "blobs":
```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![output_2_1.png](/images/output_2_1.png)

In this data set, K-means clustering works well because the two groups are circular. 
```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![output_4_1.png](/images/output_4_1.png)

## Why do we need spectral clustering?
However, when the data points do not group nicely together in a circular form, K-means fails to group the data points in the way that we would expect it to. We can see that clearly happening here with this new data:
```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
![output_4_1.png](/images/output_6_1.png)

Given this data, this is what the K-means clustering technique will output:
```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![output_4_1.png](/images/output_8_1.png)

We see above that K-means doesn't properly separate the data points, which seem to be separated in a crescent shape. This is where spectral clustering comes in! <br>

## Defining spectral clustering
According to [Wikipedia's definition](https://en.wikipedia.org/wiki/Spectral_clustering), "spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions." Let's break down this definition.


## Similarity Matrix 
Given an enumerated set of data points, the similarity matrix $$A$$ is symmetric, where $$A_{i,j}$$  represents a measure of the similarity between data points with indices $$i$$ and $$j$$. One way to measure the similarity between any two data points is by computing the pairwise distance between them. Once we calculate the pairwise distance between all data points, we can use a parameter called `epsilon` to determine if these two data points are actually similar. Given `epsilon` we can construct the similarity matrix such that for every entry $$A_{i,j}$$ in the similarity matrix $$A$$, $$A_{i,j}$$ is equal to 1 if $$X_i$$ is within the distance, `epsilon` of $$X_j$$. Otherwise, $$A_{i,j}$$ is equal to 0. Additionally, all diagonal entries, $$A_{i,i}$$ is equal to 0 since the pairwise distance between the point and itself is 0. <br>


Let's calculate the similarity matrix for the data points in this plot using `epsilon = 0.4`:
![output_4_1.png](/images/output_6_1.png)

```python
from sklearn.metrics import pairwise_distances
# initialize epsilon 
epsilon = 0.4
# calculate the distance between each datapoint in X
A = pairwise_distances(X, metric="euclidean")
# determine if each entry in A is less than epsilon^2
A = (A < epsilon).astype(int)
# fill the diagonal entries of A with 0 
np.fill_diagonal(A, 0)
# display our similarity matrix 
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])


{::options parse_block_html="true" /}
<div class="got-help">
One of my peers noticed that I had squared epsilon when calculating the similarity matrix. Fixing this to `A<epsilon` helped clear up a lot of the weird clustering I was having. 
</div>
{::options parse_block_html="false" /}

## Binary Norm Cut Objective
The similarity matrix $$A$$ contains information on which points are near each other, but we still don't know which points belong to what group. Thus, out next task is to cluster the data points in `X` by partitioning the rows and columns of $$A$$. We can do this by finding the binary norm cut objective of $$A$$. In math terms, <br>
<p align="center">
$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$
</p>

### Cut Term
In the binary norm cut objective, the cut term is referred to as $$\mathbf{cut}(C_0, C_1)$$ and is the *cut* of the clusters $$C_0$$ and $$C_1$$. Mathematically, 
<p align="center">
$$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$
</p>
What this expression means is that the cut is the sum of all weights between data points that connect one data point in cluster $$C_0$$ to cluster $$C_1$$. 

```python
def cut(A, y):
    """
    Calculates the cut of two clusters given a similarity matrix,
    A, and cluster membership, y.
    """
    # array that holds the indices of y 
    point_index = np.array(range(len(y)))
    # get the index of each data point according to its cluster 
    C1_index = point_index[y==1]
    C0_index = point_index[y==0]
    
    # add the entries from the similarity matrix for each pair of 
    # points (i,j) in different clusters
    cut_sum = 0
    for i in C1_index:
        for j in C0_index:
            cut_sum += A[i, j]
           
    return cut_sum
```
When we say that the cut term is small, it means that points in $$C_0$$ are not close to points in $$C_1$$. We can see this is the case when we compute the cut objective for the true clusters `y`. 
```python
cut(A, y)
``` 




    13




However, when we generate a random vector with the clustering labels, the cut is no longer small. This means that the distinction between the two clusters, $$C_0$$  and $$C_1$$  is not as clear since the points in  $$C_0$$ are close to $$C_1$$.
```python
# generate random cluster labels
random_y = np.random.choice([0, 1], size=200)
# calculate the cut 
cut(A, random_y)
```




    1150


<br>
### The Volume Term
The second factor in the norm cut objective is the *volume term*. 
$$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). What the expression above means is that the *volume* of cluster $$C_0$$ is a measure of the size of the cluster. Using this definition of *volume*, we can easily define a function to calculate its value.

```python
def vols(A, y):
    """
    Calculates the volume of clusters given a similarity matrix, 
    A, and clustering labels, y. 
    Input: -  A, 2D numpy array
           -  y, 1D numpy array
    Output: a tuple containing the volume of 2 clusters 
    """
    # calculate the degree of row i in the similarity matrix 
    d = np.sum(A, axis=1)
    # volume of C0 and C1 cluster
    vol_C0 = np.sum((y == 0) * d)
    vol_C1 = np.sum((y == 1) * d)
    return vol_C0, vol_C1
```
Let's see what this function outputs on our data.
```python
vol_C0, vol_C1 = vols(A, y)
vol_C0, vol_C1
```




    (2299, 2217)


{::options parse_block_html="true" /}
<div class="gave-help">
I noticed that when calling `vols(A,y)`  in several functions, one of my peers was obtaining the results by calling `v0 = vols(A,y)[0]` and `v1 = vols(A,y)[1]`. Super minor fix, but I suggested that they write `v0, v1 = vols(A, y)` instead. 
</div>
{::options parse_block_html="false" /}
We see that both volume terms are large and about the same size. We can now move on to calculating the binary norm cut objective. 

## Calculating the binary norm cut objective
Now that we have our cut term and volume term. We can finally calculate the binary norm cut objective. The binary norm cut objective allows us to find clusters such that the cut is small, and the volumes of $$C_0$$ and $$C_1$$ are large enough. As a reminder, the **binary norm cut objective of matrix $$A$$** is written mathematically as: 
<p align="center">
$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$
</p>

```python
def normcut(A, y):
    """
    Computes the binary normalized cut objective of a matrix A
    with clustering vector y
    Input: - A, a 2D numpy array representing the similarity 
                matrix
           - y, a 1D numpy array representing the clustering 
             vector
    """
    # cut term
    cut_val = cut(A,y)
    # volumes of each cluster
    v0, v1 = vols(A, y)
    return cut_val * (1/v0 + 1/v1)
```
Using our `normcut()` function, we can finally calculate the binary norm cut object of matrix $$A$$ using the true clustering labels and the randomly generated clustering labels. <br>
<br>
**Binary norm cut objective on true labels**: 
```python
normcut(A, y)
```




    0.011518412331615225



**Binary norm cut objective on random labels**:
```python
normcut(A, random_y)
```




    1.0240023597759158


We see that the `normcut` for the true labels is smaller than the `normcut` for the randomly generated labels. These results make sense since the cut term of matrix $$A$$ using the true clustering labels was a lot smaller that the cut term of matrix $$A$$ using the randomly generated clustering labels. <br>
<br>
While this is great, finding a cluster vector `y` such that `normcut(A, y)` is small is an ***NP-hard combinatorial optimization problem***. This means that it is difficult to find good clustering in a reasonable amount of time. However, we can use linear algebra to perform a math trick that makes this computation much easier. <br>
<br>
We can define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that:

<p align = "center">
$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$
</p>

Using this math trick, we can redefine the normal cut objective to be: 
<p align = "center">
$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;$$
</p>

Diving into the equation above, we can write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given $$A$$ and `y`, using the formula above. By using boolean indexing, we can easily obtain the indices where `y` is equal to 0 or 1, and set the entry at that index in the $$\mathbf{z}$$ vector to the corresponding volume value. 
```python
def transform(A, y):
    """
    Computes the approproate z vector given A and y
    Input: - A, 2D numpy array representing the similarity matrix
           - y, 1D numpy array representing the clustering vectors
    """
    # initialize the empty z vector
    z = np.zeros(len(y))
    # obtain the volumes of each cluster 
    v0, v1 = vols(A, y)
    # get the clustering labels in y that belong to C0 and set it equal to 1 / v0 
    z[y==0] = 1 / v0
    # get the clustering labels in y that belong to C1 and set it equal to -1 / v1 
    z[y==1] = -1 / v1
    return z
```

{::options parse_block_html="true" /}
<div class="gave-help">
Again, a super minor suggestion since all my peers wrote pretty efficient code (wooo!), but I suggested to them that they use boolean indexing directly when calculating their $$\mathbf{z}$$ vector. Specifically they had written
```python
mask = y == 1
z[mask] = -1/v1
```
, but I suggested that they write `z[y==1] = -1/v1` instead. This is honestly just my coding style preference though, as they both do the same thing!
</div>
{::options parse_block_html="false" /}

Let's move on to describing what the $$\mathbf{D}$$ matrix is in the equation above. $$\mathbf{D}$$  is the diagonal matrix with nonzero entries where $$d_{ii} = d_i = \sum_{j = 1}^n a_{ij}$$.
```python
# get the row sum 
d_ = np.sum(A, axis=1)
# cast the row sums along the diagonal of the matrix D
D = (np.eye(len(d_))) * d_
```
Great, we now have all the pieces to calculate the normalized cut objective using linear algebra! Let's put it all together.
```python
z = transform(A, y)
(z.T@(D - A)@z) / (z.T@D@z)
```





    0.011518412331615073




Notice that this is very close to the value we got when we called `normcut(A,y)`. This means that we can use the matrix product to approximately quantify the norm cut objective! 

```python
np.isclose(normcut(A,y), (z.T@(D - A)@z) / (z.T@D@z))
```



    True



We can also check the identity that $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ , where $$\mathbb{1}$$  is the vector of `n` (200) ones. Essentially, this means that the $$\mathbf{z}$$ vector should approximately contain the same amounts of positive and negative entries. 
```python
np.isclose(z@D@np.ones(200), 0)
```



    True



## Minimizing the normcut objective

Previously, we were tasked with minimizing the norm cut objective and we can do a similar thing with our new equation:
<p align="center">
$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$
</p>

given the condition that $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. In order to add this condition into our minimization problem, we can substiute the $$\mathbf{z}$$ vector for the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D1}$$.

```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Now, using the `minimize` function from `scipy.optimize` we can minimize the function `orth_obj` with respect to $$\mathbf{z}$$. We can also add some linear constraints to get the proper `z_` vector. 
```python
from scipy.optimize import minimize, LinearConstraint

z_ = minimize(orth_obj, x0=z, constraints=(LinearConstraint(D@np.ones(len(D)), 0, 0, keep_feasible=False)))
```
## Clustering our data
The `x` attribute of `z_` contains information about the cluster label of data point i. We can thus use it to "color" in our data points depending on which cluster it belongs to. 

```python
plt.scatter(X[:,0], X[:,1], c = z_.x < 0)
```
![output_38_1_copy.png](/images/output_38_1_copy.png)

Nice! We see that we were able to distinguish our data almost perfectly into the two crescent shaped clusters!


## Spectral clustering using eigenvalues and eigenvectors

Although this is great, minimizing the orthogonal objective is way too time intensive. Thus, we need a better way to solve this problem. Luckily for us, we can use linear algebra, specifically by calculating eigenvalues and eigenvectors of matrices, to solve this problem. Here are the following steps to do that:
1. Construct the Laplacian matrix of the similarity matrix $$A$$.  
<p align="center">
$$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$
</p>
2. Find the eigenvector corresponding to its second-smallest eigenvalue (`z_eig`)
3. Plot the data using the sign of `z_eig` as the color


```python
# construct the Laplacian matrix
L = np.linalg.inv(D)@(D-A)
# compute eigenvalues and eigen vectors 
eig_values, eig_vectors = np.linalg.eig(L)
# get the index with the second smallest eigenvalue and corresponding eigenvector 
second_smallest_idx = np.argsort(eig_values, axis=-1)[1]
z_eig = eig_vectors[:, second_smallest_idx] 
```


```python
# plot the data using the sign of `z_eig` as the color
plt.scatter(X[:,0], X[:,1], c = z_eig < 0 )
```
![output_41_1_copy.png](/images/output_41_1_copy.png)
    
Again, we see that we are almost able to cluster the data into the two different crescents!

## Putting spectral clustering into one function
Now, we will put everything together into a function called `spectral_clustering(X, epsilon)`. In this function, `X` represents the input data and `epsilon` represents the distance threshold. This function will perform spectral clustering by returning an array of binary labels (0 or 1) to each data point `i`. To summarize the steps that we have taken to get this far, we need to:
1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def get_similarity_matrix(X, epsilon):
    """
    Constructs the similarity matrix given input data and distance threshold
    Input: - X, a 2D numpy array where each entry represents the coordinates of each datapoint
           - epsilon, an int representing the distance threshold
    """
    A = pairwise_distances(X, metric="euclidean")
    A = (A < epsilon).astype(int)
    np.fill_diagonal(A, 0)
    return A

def get_laplacian(A):
    """
    Constructs the Laplcaian matrix given a smilarity matrix A
    """
    d_ = np.sum(A, axis=1)
    D = np.eye(len(d_)) * d_
    laplacian = np.linalg.inv(D)@(D-A)
    return laplacian

def get_eigen_vector(laplacian):
    """
    Retrieves the eigenvector corresponding to the second smallest eigenvalue 
    given a Laplacian matrix 
    """
    _eig_values, _eig_vectors = np.linalg.eig(laplacian)
    idx = np.argsort(_eig_values, axis=-1)[1]
    _z_eig = _eig_vectors[:, idx] 
    return _z_eig

def spectral_clustering(X, epsilon):
    """
    Performs spectral clustering by using the Rayleigh-Ritz Theorem.
    Input: - X, a 2D numpy array where each entry represents the coordinates of the datapoint
           - epsilon, an int representing the distance threshold
    Output: an array of binary labels (0 or 1) to each data point i
    """
    A = get_similarity_matrix(X, epsilon)
    L = get_laplacian(A)
    _z_eig = get_eigen_vector(L)
    return _z_eig
```
{::options parse_block_html="true" /}
<div class="got-help">
One of my peer noticed that I had a mistype and accidentally calculated the pseudo inverse of `(D)@(D-A)` in my `get_laplacian(A)` function. I really appreciated them explaining the difference between pseudo inverse and the normal inverse of a matrix and how it might affect my results. 
</div>
{::options parse_block_html="false" /}

## Testing our spectral clustering function on different data sets
We can generate different data sets using `make_moons` and test our spectral clustering function. Since `spectral_clustering(X, epsilon)` returns an array of the the binary labels, we can pass it into the `color` argument of `plt.scatter()`.


### Moon Data Set
Let's see what happens when we increase the noise from 0.05 to 0.075 of our input data `X` from before. First we must generate our data: 
```python
np.random.seed(1)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.075, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    
![png](/images/output_39_1.png)
    
Now, we can test out our `spectral_clustering()` function by specifying it as the `c` parameter in `plt.scatter()` to color in each data point according to its cluster. 

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon=0.48))
```


    
![png](/images/output_47_1.png)
    
From the above figure, we see that the clustering still works out pretty well! Cool!


### Bull's Eye Data
Now, let's try our spectral clustering function on another data set -- the bull's eye! We can make this bull's eye data using the `make_circles` method of `datasets`.


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

    
![png](/images/output_42_1.png)
    


Again K-means clustering will not do well here because there are two concentric circles.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![png](/images/output_44_1.png)
    

K-means clustering does not work well on this dataset, but what about our spectral clustering? Using an `epsilon` of 0.4, we see that we are able to pretty accurately distinguish the 2 different clusters, i.e., the inner circle versus the outer circle.  
```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon=0.4))
```

    
![png](/images/output_46_1.png)
    
We can test different `epsilon` values to see how it affects our clustering. Let's first increase it to 0.6.
```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon=0.6))
```
![png](/images/output_48_1.png)

We can see above that the clustering is not very accurate. In fact, it give us something that K-means clustering would output. Let's try lowering our `epsilon` value to 0.2.

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon=0.2))
```
![png](/images/output_50_1.png)

Again, not quite what we wanted. Looks like an `epsilon` value of 0.4 works the best!

## Conclusion
To recap, we can use spectral clustering as an approach to cluster data without making assumptions about its shape, which is great for data that are not circular by nature. Using spectral clustering, we can find out a lot of information about what is related and what is not and infer different things based on this premise. Spectral clustering is heavily used in all sorts of fields and is a great way to start off some exploratory data analysis. 